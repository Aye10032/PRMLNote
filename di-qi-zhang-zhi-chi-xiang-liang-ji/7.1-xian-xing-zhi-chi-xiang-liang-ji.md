# 7.1 线性支持向量机

## 7.1.1 间隔

### 一、函数间隔

对于一个训练样本$$(\mathbf x^i,y^i)$$，它到$$(\mathbf w,b)$$确定的超平面的<mark style="color:purple;">**函数间隔**</mark>为：
$$
\hat{\gamma}^i = y^i(\mathbf w^T\mathbf x^i+b)
$$
<mark style="color:orange;">**函数间隔与距离是正相关的**</mark>



- $$y^i=1$$，$$(\mathbf w^T\mathbf x^i + b)$$是一个大的正数
- $$y^i=-1$$，$$(\mathbf w^T\mathbf x^i + b)$$是一个比较小的负数
- $$y^i(\mathbf w^T\mathbf x^i+b)>0$$，说明模型对样本的预测是正确的
- <mark style="color:red;">**大的函数间隔→高的预测置信度**</mark>



对于训练数据集$$S = \{(\mathbf x^i,y^i),\ i=1,\dots,N\}$$，它的函数间隔定义为所有样本中<mark style="color:orange;">**最小的**</mark>那个：
$$
\hat{\gamma} = \min_i \hat{\gamma}^i,\ i=1,\dots,N
$$


### 二、几何间隔

![](../.gitbook/assets/7.1.1.png)

对于样本$$(\mathbf x^i,y^i)$$，$$y^i=1$$，它到决策面的距离$$\gamma^i$$是线段AB的长度

其中，点B可以表示为：
$$
\mathbf x^i - \frac{\gamma^i\mathbf w}{\Vert \mathbf w\Vert_2}
$$
由于点B在<mark style="color:purple;">**决策边界**</mark>上，即：
$$
\mathbf w^T\left(\mathbf x^i - \frac{\gamma^i\mathbf w}{\Vert \mathbf w\Vert_2}\right) + b = 0
$$
求解此方程可以得到样本$$(\mathbf x^i,y^i)$$的<mark style="color:purple;">**几何间隔**</mark>为：
$$
\gamma^i = y^i\left(\left(\frac{\mathbf w}{\Vert\mathbf w\Vert_2}\right)^T\mathbf x^i + \frac{b}{\Vert\mathbf w\Vert_2}\right)
$$
同样的，对于训练数据集$$S = \{(\mathbf x^i,y^i),\ i=1,\dots,N\}$$，关于判别界面的几何间隔为：
$$
\gamma = \min_i \gamma^i,\ i=1,\dots,N
$$


**函数间隔与几何间隔的关系**：
$$
\gamma^i = \frac{\hat{\gamma}^i}{\Vert \mathbf w\Vert_2}
\\
\gamma = \frac{\hat{\gamma}}{\Vert \mathbf w\Vert_2}
$$
显然，若$$\Vert \mathbf w\Vert_2=1$$，则二者相等



<mark style="color:red;">**几何间隔具有不变性**</mark>



### 三、最优间隔分类器

假设数据是<mark style="color:orange;">**线性可分**</mark>的

给定一个训练集，一个自然的想法是试图找到一个使<mark style="color:orange;">**几何间隔最大化**</mark>的决策边界，这表示对训练集的有可信的预测并且对训练数据的良好“拟合”

![](../.gitbook/assets/7.1.2.png)

那么就需要最大化间隔，即：
$$
\begin{align}
\max_{\gamma,\mathbf w,b}\ &\gamma \nonumber
\\
s.t.\ &y^i\left(\left(\frac{\mathbf w}{\Vert \mathbf w\Vert_2}\right)^T\mathbf x^i + \frac{b}{\Vert \mathbf w\Vert_2}\right)\geq\gamma,i=1,\dots,N
\end{align}
$$
可以将问题转化为<mark style="color:purple;">**几何间隔**</mark>：
$$
\begin{align}
\max_{\gamma,\mathbf w,b}\ &\frac{\hat{\gamma}}{\Vert \mathbf w\Vert_2} \nonumber
\\
s.t.\ &y^i(\mathbf w^T\mathbf x^i+b)\geq\hat\gamma,i=1,\dots,N
\end{align}
$$
进一步简化问题，**令几何间隔为单位1**：
$$
\begin{align}
\min_{\mathbf w,b}\ &\frac12\Vert\mathbf w\Vert_2^2 \nonumber
\\
s.t.\ &y^i(\mathbf w^T\mathbf x^i+b)\geq1,i=1,\dots,N
\end{align}
$$

也就是说，在分类正确的情况下，样本到判别界面的距离应当大于单位1





## 7.1.2 拉格朗日对偶性

### 一、一般情况

对于一般的含有等式和不等式约束的最优化问题：
$$
\begin{align}
&\min_{w} \quad f(w) \tag{1}
\\
&\begin{array}
{r@{\quad} l@{\quad}l}
s.t. &g_i(w)\leq0, &i=1,\dots,k
\\
 &h_i(w) = 0, &i=1,\dots,l
\end{array}
\end{align}
$$
可以定义<mark style="color:purple;">**广义拉格朗日函数**</mark>：
$$
L(w,\alpha,\beta) = f(w) + \color{blue}\sum_{i=1}^k \color{red}{\alpha_i} \color{blue}g_i(w) + \sum_{i=1}^l\color{red}{\beta_i} \color{blue}h_i(w)
$$
 上式中，$$\alpha_i$$和$$\beta_i$$分别是等式约束和不等式约束的<mark style="color:purple;">**拉格朗日乘子**</mark>，并要求$$\color{red}\alpha_i\geq0$$

那么考虑对于$$w$$的函数：
$$
\theta_P(w) = \max_{\alpha,\beta;\alpha_i\geq0}{L(w,\alpha,\beta)}
$$
这里下标P表示<mark style="color:orange;">**原始问题**</mark>

假设存在$$w_i$$使得约束条件不成立（即$$g_i(w)>0$$或$$h_i(w)\neq0$$），则可以通过令$$\alpha_i$$或$$\beta_i$$等于正无穷来使得$$\theta_P(w)=+\infin$$

而若$$w$$满足全部约束，显然可以有$$\theta_P(w) = f(w)$$，即：
$$
\theta_P(w) = 
\begin{cases}
f(w) & w满足约束
\\
+\infin & 其它
\end{cases}
$$
那么如果考虑最小问题：
$$
\min_{w}\quad\theta_P(w) = \min_{w}\max_{\alpha,\beta;\alpha_i\geq0}{L(w,\alpha,\beta)}
$$
它与原始最优化问题是等价的，问题$$\min\limits_{w}\max\limits_{\alpha,\beta;\alpha_i\geq0}{L({w,\alpha,\beta})}$$称为<mark style="color:purple;">**广义拉格朗日函数的极小极大问题**</mark>。这样就将原始问题的最优解转化为了拉格朗日函数的极小极大问题。定义原始问题的最优值
$$
p^* = \min_{x}\quad\theta_P(w)
$$
为<mark style="color:purple;">**原始问题的值**</mark>



### 二、对偶问题

 定义：
$$
\theta_D(w)
$$




## 7.1.3 线性SVM

<mark style="color:red;">**支持向量**</mark>：距分离超平面<mark style="color:orange;">**最近**</mark>的样本



- **输入**：<mark style="color:orange;">**线性可分**</mark>的数据集$$S=\{(\mathbf x^i,y^i),i=1,\dots,N\}$$
- **输出**：判别函数及决策/判别界面
- **最优化问题**

$$
\begin{align}
\min_{\mathbf w,b}\ &\frac12\Vert\mathbf w\Vert_2^2 \nonumber
\\
s.t.\ &y^i(\mathbf w^T\mathbf x^i+b)\geq1,i=1,\dots,N
\end{align}
$$

- **分离超平面**：$$(\mathbf w^*)^T\mathbf x + b^* = 0$$
- **判别函数**：$$f_{\mathbf w,b}(\mathbf x) = sign((\mathbf w^*)^T\mathbf x + b^*)$$



{% hint style="success" %}

**理论保证**：对于线性可分的训练数据集，最大间隔分类器存在且唯一

{% endhint %}





## 参考

[https://www.kaggle.com/code/alaapdhall/custom-svm-with-numpy-vs-sklearn](https://www.kaggle.com/code/alaapdhall/custom-svm-with-numpy-vs-sklearn)
