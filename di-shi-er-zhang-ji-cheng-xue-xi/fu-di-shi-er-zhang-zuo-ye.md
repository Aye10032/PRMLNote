# 附 第十二章作业

## 作业1

### 题目

模型复杂度过低/过高通常会导致Bias和Variance怎样的问题？

### 答

- 模型简单会出现欠拟合，表现为偏差高，方差低

  - 当模型太简单，不能捕捉到数据中的复杂结构时，模型往往会出现⾼偏 差。这意味着模型在训练数据上的表现和在未知数据上的表现都不太好，因为它 没有很好地学习到数据的特征，从⽽导致错误的预测或分类

  - 由于模型简单，它对训练数据的⼩变化不太敏感，因此在不同的数据集 上的表现⽐较⼀致，导致低⽅差。但这种⼀致性是以牺牲准确性为代价的

- 模型复杂会出现过拟合，表现为偏差低，方差高
  - ⼀个复杂的模型能够很好地适应训练数据，⼏乎完美地捕获其所有特 征，从⽽在训练数据上表现出很低的偏差。它可以⾮常精确地预测训练数据中的 结果
  - 过于复杂的模型可能会对训练数据中的噪声和误差也进⾏学习， 这导致它对于新的、未⻅过的数据表现出⾼⽅差。这意味着模型在不同的数据集 上可能表现出很⼤的波动，即使这些数据集之间的差异很⼩





## 作业2

### 题目

怎样判断、怎样缓解过拟合/欠拟合问题？

### 答

#### 判断

从理论上看，若模型的偏差高、方差低，则意味着存在欠拟合；反之则存在过拟合

实际可以通过校验误差判断。校验误差随着模型复杂度的变化先减小，此时模型处于欠拟合状态；当模型复杂度超过一定值后，校验误差随模型复杂度增加而增大 ，此时模型进入过拟合状态

看在训练集和测试集上的表现

#### 缓解

- **欠拟合**
  - 需要增减模型的复杂度
  - 增加训练时间
  - 减少正则化
- **过拟合**
  - 降低模型的复杂度
  - 扩大训练集
  - 添加正则项
  - 神经网络中增加Dropout



## 作业3

### 题目

比较Bagging和Boosting算法的异同

### 答

- **相同**
  - 都是集成学习的算法，本质思路都是通过组合多个弱学习器来构建一个强学习器
  - 都是旨在通过集成的方法降低泛化误差

- **不同**
  - **训练方式**：
    - bagging是并行的
    - boosting是顺序执行的

  - **主要目标**
    - bagging旨在降低方差，防止出现过拟合
    - boosting旨在降低偏差，提高模型在训练集上的表现

  - **权重**
    - bagging样本权重是一样的
    - boosting会对分类错误的样本增加权重

  - **对噪声的敏感度**
    - bagging模型之间是独立的，容错性更强
    - boosting对异常值更敏感






## 作业4

### 题目

简述Adaboosting的流程

### 答

首先，用一个基础的学习器对数据集进行分类训练。接下来的每一次迭代中，增加分类错误的数据的权重，减轻分类正确的样本的权重，依此训练下一个分类器。

最后，对于这一系列弱分类器，若该分类器错误率高，则权重较低；反之则权重较高。依此进行加权求和，得到最终的分类结果。



## 作业5

### 题目

随机森林更适合采用那种决策树？

- A、性能好，深度较深
- B、性能弱、深度较浅 

### 答

**A**：较深的决策树容易发生过拟合问题，然而采用Bagging可以降低模型的方差，因此可以较好的缓解该问题



## 作业6

### 题目

基于树的Boosting更适合采用那种决策树？

- A、性能好，深度较深
- B、性能弱、深度较浅

### 答

**B**：boosting是将许多弱学习器进行组合，形成强分类器。因此此时选择层数不深的决策树即可





## 作业7

### 题目

如果对决策树采用Bagging方式进行集成学习，更适合采用哪种方法对决策树的超参（如树的深度）进行调优？

- A、交叉验证
- B、包外估计

### 答

**B**：在Bagging中，每个基学习器只在原始数据集的一部分上训练，所以可以不用交叉验证，直接采用包外估计
